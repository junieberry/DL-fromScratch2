{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jDaSjy7j58AHrCPEkxa_esffgMurN4R2",
      "authorship_tag": "ABX9TyN5fLBZC3aXMQS6JhGeLS7V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/DL-fromScratch2/blob/main/07_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWbtv2X2xF-O",
        "outputId": "af899127-2c42-4fbe-e9e8-48a59dca9cdf"
      },
      "source": [
        "cd /content/drive/MyDrive/밑시딥/deep-learning-from-scratch-2/ch07"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/밑시딥/deep-learning-from-scratch-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxHd5f1yz4Zg"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import sequence\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfWK2ZWzyFZO",
        "outputId": "f3fde0d7-83fa-4745-e3b2-19cada9a6582"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 7) (45000, 5)\n",
            "(5000, 7) (5000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqyXVTipykCs"
      },
      "source": [
        "class Encoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V,D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H)/np.sqrt(D)).astype('f')\n",
        "    lstm_Wh = (rn(H, 4*H)/np.sqrt(H)).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "\n",
        "    self.params = self.embed.params + self.lstm.params\n",
        "    self.grads = self.embed.grads + self.lstm.grads\n",
        "    self.hs = None\n",
        "  \n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)\n",
        "    self.hs = hs\n",
        "    return hs[:, -1, :]\n",
        "  \n",
        "  def backward(self,dh):\n",
        "    dhs = np.zeros_like(self.hs)\n",
        "    dhs[:, -1, :] = dh\n",
        "\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PTtEuWpz-zd"
      },
      "source": [
        "class Decoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V,D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H)/np.sqrt(D)).astype('f')\n",
        "    lstm_Wh = (rn(H, 4*H)/np.sqrt(H)).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "    affine_W = (rn(H,V)/np.sqrt(H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh,lstm_b, stateful=True)\n",
        "    self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in (self.embed, self.lstm, self.affine):\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, h):\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    out = self.embed.forward(xs)\n",
        "    out = self.lstm.forward(out)\n",
        "    score = self.affine.forward(out)\n",
        "    return score\n",
        "  \n",
        "  def backward(self, dscore):\n",
        "    dout = self.affine.backward(dscore)\n",
        "    dout = self.lstm.backward(dout)\n",
        "    dout = self.embed.backward(dout)\n",
        "    dh = self.lstm.dh\n",
        "  \n",
        "  def generate(Self, h, start_id, sample_size):\n",
        "    sampled = []\n",
        "    sample_id = start_id\n",
        "    self.lstm,set_state(h)\n",
        "\n",
        "    for _ in range(sample_size):\n",
        "      x = np.array(sample_id).reshape((1,1))\n",
        "      out = self.embed.forward(X)\n",
        "    \n",
        "    out = self.lstm.forward(out)\n",
        "    score = self.affine.forward(out)\n",
        "\n",
        "    sample_id = np.argmax(score.flatten())\n",
        "    sampled.append(int(sample_id))\n",
        "  \n",
        "    return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycm_WdYg1nXU"
      },
      "source": [
        "class Seq2seq(BaseModel):\n",
        "  def __init__(self, wocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    self.encoder = Encoder(V, D, H)\n",
        "    self.decoder = Decoder(V, D, H)\n",
        "    self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "    self.params = self.encoder.params + self.decoder.params\n",
        "    self.grads = self.encoder.grads + self.decoder.grads\n",
        "  \n",
        "  def forward(self, xs, ts):\n",
        "    decoder_xs, decoder_ts = ts[:,:-1], ts[:,1:]\n",
        "    h = self.encoder.forward(xs)\n",
        "    score = self.decoder.forward(decoder_xs, h)\n",
        "    loss =self.softmax.forward(score, decoder_ts)\n",
        "    return loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    dout = self.softmax.backward(dout)\n",
        "    dh = self.decoder.backward(dout)\n",
        "    dout = self.encoder.backward(dh)\n",
        "    return dout\n",
        "  \n",
        "  def generate(self, xs, start_id, sample_size):\n",
        "    h = self.encoder.forward(xs)\n",
        "    sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "    return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtoKSm9f2uAw"
      },
      "source": [
        "1. 학습 데이터에서 미니배치 선택\n",
        "2. 미니배치에서 기울기 계산\n",
        "3. 기울기 사용해서 매개변수 갱신"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cnmC67O2sDL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "# from peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "## 데이터셋 읽기\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "## 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "max_epoch = 25\n",
        "max_grad = 5.0\n",
        "\n",
        "## 모델, 옵티마이저, 트레이너\n",
        "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_lost = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
        "  \n",
        "  correct_num = 0\n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]\n",
        "    verbose = i<10\n",
        "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
        "  \n",
        "  acc = float(correct_num)/len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('검증 정확도 %.3f%%' %(acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}