{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improvedCBOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eWkblBXeloJFm_2L_LqZz-9DKF1W3LDe",
      "authorship_tag": "ABX9TyMzjN8lHdCUkAkamX6uE33/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/DL-fromScratch2/blob/main/improvedCBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YWPLYIabmW2",
        "outputId": "840e18d1-aaa1-470a-f25a-cf3c5df092ff"
      },
      "source": [
        "cd /content/drive/MyDrive/밑시딥/deep-learning-from-scratch-2/ch04"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/밑시딥/deep-learning-from-scratch-2/ch04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZHR_jKqb2ce"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from negative_sampling_layer import UnigramSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQF87Yt1jdqx"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Byo6sjbh_Mu"
      },
      "source": [
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "  \n",
        "  def forward(self, idx):\n",
        "    W, = self.params ## 튜플\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "  \n",
        "\n",
        "  ## \n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dw[...] = 0\n",
        "\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "      dW[word_id] += dout[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAgpd0sRttrx"
      },
      "source": [
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None ## forward 시 계산 결과 유지\n",
        "  \n",
        "  ## h는 은닉층 뉴런\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx)\n",
        "    out = np.sum(target+W * h, axis=1)\n",
        "\n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpgPomYrqNvV",
        "outputId": "050662e3-7296-46d0-a3d7-1fe45bcf1202"
      },
      "source": [
        "W = np.arange(21).reshape(7,3)\n",
        "params=[W]\n",
        "w, = params\n",
        "print(w)\n",
        "w=params\n",
        "print(w)\n",
        "\n",
        "\n",
        "a=np.array([[0,1,2],[9,10,11],[3,4,5]])\n",
        "\n",
        "print(np.sum(a))\n",
        "print(np.sum(a, axis=0))\n",
        "print(np.sum(a, axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]\n",
            " [15 16 17]\n",
            " [18 19 20]]\n",
            "[array([[ 0,  1,  2],\n",
            "       [ 3,  4,  5],\n",
            "       [ 6,  7,  8],\n",
            "       [ 9, 10, 11],\n",
            "       [12, 13, 14],\n",
            "       [15, 16, 17],\n",
            "       [18, 19, 20]])]\n",
            "45\n",
            "[12 15 18]\n",
            "[ 3 30 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNX1MrNo1MBc"
      },
      "source": [
        "## Negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recAC8Fd6WUY"
      },
      "source": [
        "from common.layers import SigmoidWithLoss\n",
        "\n",
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "    self.sample_size = sample_size\n",
        "    self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "\n",
        "    ## 0번째 계층은 positive sample을 위한 계층\n",
        "    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size +1)]\n",
        "    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.embed_dot_layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "  \n",
        "  def forward(self, h, target):\n",
        "    batch_size = target.shape[0]\n",
        "    negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "    ## forward positive sample\n",
        "    score = self.embed_dot_layers[0].forward(h, target)\n",
        "    correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "    loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "    ## forward negative sample\n",
        "    negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "    for i in range(self.sample_size):\n",
        "      negative_target = negative_sample[:,i]\n",
        "      score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
        "      loss += self.loss_layers[1+i].forward(score, negative_label)\n",
        "    return loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    dh = 0\n",
        "    for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "      dscore = l0.backward(dout)\n",
        "      dh += l1.backward(dscore)\n",
        "    \n",
        "    return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U03RAolo2kp1",
        "outputId": "1bd74041-f46a-4a3f-cff1-1a2547a0850d"
      },
      "source": [
        "\n",
        "## UnigramSampler 예시\n",
        "\n",
        "corpus = np.array([0,1,2,3,4,1,2,3])\n",
        "power=0.75\n",
        "sample_size=2\n",
        "\n",
        "## power = 확률분포에 제곱할 값 (기본값=0.75)\n",
        "## sample_size = 네거티브 샘플링할 수\n",
        "sampler=UnigramSampler(corpus, power, sample_size)\n",
        "## target = 긍정적 얘시\n",
        "target =np.array([1,3,0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 4]\n",
            " [2 0]\n",
            " [3 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OojRyZIN1zV_",
        "outputId": "e66e2862-4d1c-4483-a97a-6667890f2549"
      },
      "source": [
        "\n",
        "\n",
        "## 0-9 중 하나 무작위로 샘플링\n",
        "print(np. random.choice(10))\n",
        "\n",
        "## words에서 하나 무작위로 샘플링\n",
        "word=['a','b','c','d','e','f']\n",
        "print(np.random.choice(word))\n",
        "\n",
        "## word에서 2개 무작위로 샘플링 (중복 o)\n",
        "print(np.random.choice(word, size=2))\n",
        "\n",
        "## word에서 2개 무작위로 샘플링 (중복 x)\n",
        "print(np.random.choice(word, size=2, replace=False))\n",
        "\n",
        "## 확률 분포에 따라 샘플링\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "print(np.random.choice(word,p=p))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "b\n",
            "['d' 'd']\n",
            "['c' 'a']\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702AqpL6qLZc"
      },
      "source": [
        "## CBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmggilcNcAE9"
      },
      "source": [
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "class CBOW:\n",
        "  def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "    V,H = vocab_size, hidden_size\n",
        "\n",
        "    ## 가중치를 32비트 부동소수점 수로 초기화\n",
        "    W_in=0.01*np.random.randn(V, H).astype('f')\n",
        "    W_out=0.01*np.random.randn(H, V).astype('f')\n",
        "\n",
        "    ## 계층 생성\n",
        "    ## 입력 측 맥락은 윈도우 크기만큼 생성\n",
        "    self.in_layers = []\n",
        "    for i in range(2*window_size):\n",
        "      layer = Embedding(W_in)\n",
        "      self.in_layers.append(layer)\n",
        "    self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "    ## 가중치와 기울기\n",
        "    layers = self.in_layers + [self.ns_loss]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "    \n",
        "    \n",
        "    ## 단어의 분산 표현 저장\n",
        "    self.word_vecs = W_in\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "    h = 0\n",
        "    for i, layer in enumerate(self.in_layers):\n",
        "      h += layer.forward(contexts[:,i])\n",
        "    h *= 1/len(self.in_layers)\n",
        "    loss = self.ns_loss.forward(h, target)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.ns_loss.backward(dout)\n",
        "    dout *= 1/len(self.in_layers)\n",
        "    for layer in self.in_layers:\n",
        "      layer.backward(dout)\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbjPyqADcKsf"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common import config\n",
        "\n",
        "# config.GPU = True\n",
        "\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb\n",
        "\n",
        "## Hyperparameter\n",
        "window_size = 5\n",
        "hidden_size = 1000\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "## Data preprocessing\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "# if config.GPU:\n",
        "#   contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "##\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "## Training\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "  word_vecs = to_cpu(word_vecs)\n",
        "params={}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file='CBOW_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "  pickle.dump(params, f,-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}